{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde8a9dc",
   "metadata": {},
   "source": [
    "# LangWatch Batch Evaluation Cookbook\n",
    "\n",
    "## Step 1: Define our LLM pipeline\n",
    "\n",
    "Let's create a simple RAG pipeline using LangChain, guaranteeing that we can get the output and the retrieved documents used during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a61c2b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/var/folders/rp/9_s_f3kd1ssb089myww_p9zw0000gn/T/ipykernel_93674/189265682.py:37: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_documents = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "retrieved_documents: ['Introduction - LangWatchLangWatch home pageSearch...SupportDashboardlangwatch/langwatchlangwatch/langwatchSearch...NavigationIntroductionOpen DashboardGitHub RepoIntroductionWelcome to LangWatch, the all-in-one open-source LLMops platform.\\nLangWatch allows you to track, monitor, guardrail and evaluate your LLMs apps for measuring quality and alert on issues.\\nFor domain experts, it allows you to easily sift through conversations, see topics being discussed and annotate and score messages\\nfor improvement in a collaborative manner with the development team.\\nFor developers, it allows you to debug, build datasets, prompt engineer on the playground and\\nrun batch evaluations or DSPy experiments to continuously improve the product.\\nFinally, for the business, it allows you to track conversation metrics and give full user and quality analytics, cost tracking, build\\ncustom dashboards and even integrate it back on your own platform for reporting to your customers.', 'custom dashboards and even integrate it back on your own platform for reporting to your customers.\\nYou can sign up and already start the integration on our free tier by following the guides bellow:\\nPython Integration GuideTypeScript Integration GuideREST API\\nYou can also open the demo project check out a video on our platform.\\n\\u200bGet in touch\\nFeel free to reach out to us directly at [email\\xa0protected]. You can also open a GitHub issue\\nto report bugs and request features, or join our Discord channel and ask questions directly for the community and the core team.githublinkedinPowered by MintlifyOn this pageGet in touch']\n",
      "output: LangWatch is an open-source LLMops platform for tracking, monitoring, and evaluating LLM applications, enhancing quality and user analytics.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_core.vectorstores.base import VectorStoreRetriever\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.langwatch.ai\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "\n",
    "vector = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "retrieved_documents = []\n",
    "\n",
    "# Wrap the FAISS retriever so that we can capture which documents were used to generate the response\n",
    "@tool\n",
    "def langwatch_search(\n",
    "    query: str\n",
    ") -> list[Document]:\n",
    "    \"\"\"\"Search for information about LangWatch. For any questions about LangWatch, use this tool if you didn't already\"\"\"\n",
    "\n",
    "    global retrieved_documents\n",
    "    retrieved_documents = retriever.get_relevant_documents(query)\n",
    "    return retrieved_documents\n",
    "\n",
    "tools = [langwatch_search]\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that only reply in short tweet-like responses, use tools only once.\\n\\n{agent_scratchpad}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "agent = create_tool_calling_agent(model, tools, prompt)\n",
    "executor = AgentExecutor(agent=agent, tools=tools, verbose=False)  # type: ignore\n",
    "\n",
    "output = executor.invoke({\"question\": \"What is LangWatch?\"})[\"output\"]\n",
    "\n",
    "print(\"\")\n",
    "print(\"retrieved_documents:\", [ d.page_content for d in retrieved_documents])\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac02587",
   "metadata": {},
   "source": [
    "## Step 2: Run the Batch Evaluation Experiment\n",
    "\n",
    "Now we can use the dataset we have from LangWatch to run a batch evaluation experiment through our LLM pipeline, to see the results and tweak it for optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78f4041d-b469-42d9-affc-f7ccbc03c98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langwatch.batch_evaluation import BatchEvaluation, DatasetEntry\n",
    "\n",
    "\n",
    "def callback(entry: DatasetEntry):\n",
    "    output = executor.invoke({\"question\": entry.input})[\"output\"]\n",
    "\n",
    "    return {\"output\": output, \"contexts\": [d.page_content for d in retrieved_documents]}\n",
    "\n",
    "\n",
    "# Instantiate the BatchEvaluation object\n",
    "evaluation = BatchEvaluation(\n",
    "    dataset=\"langwatch-rag\",\n",
    "    evaluations=[\"openai-moderation\",\"faithfulness\"],\n",
    "    callback=callback,\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "results = evaluation.run()\n",
    "results.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc189990-4274-4194-b321-eb07b5231a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
